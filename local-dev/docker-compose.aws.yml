version: '3.8'
name: crawlinghub-aws

# ===============================================
# CrawlingHub - AWS 리소스 연결용 로컬 개발 환경
# ===============================================
# 사용법:
# 1. AWS SSM Session Manager로 RDS/Redis 포트 포워딩 설정
#    ./scripts/aws-port-forward.sh
# 2. docker-compose -f docker-compose.aws.yml up -d
#
# 포트 포워딩:
#   - localhost:13307 -> RDS (prod-shared-mysql)
#   - localhost:16379 -> ElastiCache (crawlinghub-redis-prod)
# ===============================================

services:
  # ===============================================
  # Application Services (AWS 리소스 연결)
  # ===============================================

  web-api:
    build:
      context: ..
      dockerfile: bootstrap/bootstrap-web-api/Dockerfile
    container_name: crawlinghub-web-api-aws
    env_file:
      - .env.aws
    environment:
      SPRING_PROFILES_ACTIVE: prod
      SERVER_PORT: 8080
      DB_HOST: host.docker.internal
      DB_PORT: 13307
      DB_PASSWORD: "${DB_PASSWORD}"
      REDIS_HOST: host.docker.internal
      REDIS_PORT: 16379
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "8080:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - crawlinghub-network
    restart: unless-stopped

  # ===============================================
  # Scheduler Service
  # ===============================================
  scheduler:
    build:
      context: ..
      dockerfile: bootstrap/bootstrap-scheduler/Dockerfile
    container_name: crawlinghub-scheduler-aws
    env_file:
      - .env.aws
    environment:
      SPRING_PROFILES_ACTIVE: prod
      SERVER_PORT: 8081
      DB_HOST: host.docker.internal
      DB_PORT: 13307
      DB_PASSWORD: "${DB_PASSWORD}"
      REDIS_HOST: host.docker.internal
      REDIS_PORT: 16379
      # SQS Queue URLs
      AWS_SQS_LISTENER_CRAWL_TASK_QUEUE_URL: https://sqs.ap-northeast-2.amazonaws.com/646886795421/prod-monitoring-sqs-crawlinghub-crawling-task
      AWS_SQS_LISTENER_EVENT_BRIDGE_TRIGGER_QUEUE_URL: https://sqs.ap-northeast-2.amazonaws.com/646886795421/prod-monitoring-sqs-crawlinghub-eventbridge-trigger
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "8081:8081"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - crawlinghub-network
    restart: unless-stopped

  # ===============================================
  # Crawl Worker Service
  # ===============================================
  worker:
    build:
      context: ..
      dockerfile: bootstrap/bootstrap-crawl-worker/Dockerfile
    container_name: crawlinghub-worker-aws
    env_file:
      - .env.aws
    environment:
      SPRING_PROFILES_ACTIVE: prod
      SERVER_PORT: 8082
      DB_HOST: host.docker.internal
      DB_PORT: 13307
      DB_PASSWORD: "${DB_PASSWORD}"
      REDIS_HOST: host.docker.internal
      REDIS_PORT: 16379
      # SQS Queue URLs
      AWS_SQS_LISTENER_CRAWL_TASK_QUEUE_URL: https://sqs.ap-northeast-2.amazonaws.com/646886795421/prod-monitoring-sqs-crawlinghub-crawling-task
      AWS_SQS_LISTENER_CRAWL_TASK_DLQ_URL: https://sqs.ap-northeast-2.amazonaws.com/646886795421/prod-monitoring-sqs-crawlinghub-crawling-task-dlq
      AWS_SQS_LISTENER_EVENT_BRIDGE_TRIGGER_QUEUE_URL: https://sqs.ap-northeast-2.amazonaws.com/646886795421/prod-monitoring-sqs-crawlinghub-eventbridge-trigger
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "8082:8082"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - crawlinghub-network
    restart: unless-stopped

# ===============================================
# Networks
# ===============================================
networks:
  crawlinghub-network:
    driver: bridge
