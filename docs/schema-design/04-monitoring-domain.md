# ğŸ“Š ëª¨ë‹ˆí„°ë§ ë„ë©”ì¸ (Monitoring Domain) ìŠ¤í‚¤ë§ˆ ì„¤ê³„

## ğŸ“Œ ê°œìš”

ëª¨ë‹ˆí„°ë§ ë„ë©”ì¸ì€ í¬ë¡¤ë§ ì‹œìŠ¤í…œì˜ ìƒíƒœ, ì„±ëŠ¥, í’ˆì§ˆì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ì—ëŸ¬ ì¶”ì , ì•Œë¦¼ ê´€ë¦¬, ëŒ€ì‹œë³´ë“œ ì§€ì› ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

### í•µì‹¬ ê¸°ëŠ¥
- **ì„±ëŠ¥ ë©”íŠ¸ë¦­**: í¬ë¡¤ë§ ì†ë„, ì„±ê³µë¥ , ì‘ë‹µ ì‹œê°„
- **ì—ëŸ¬ ì¶”ì **: ì—ëŸ¬ ë¡œê·¸ ë° íŒ¨í„´ ë¶„ì„
- **ë°ì´í„° í’ˆì§ˆ**: ìˆ˜ì§‘ ë°ì´í„°ì˜ ì™„ì „ì„± ê²€ì¦
- **ì‹¤ì‹œê°„ ì•Œë¦¼**: ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼
- **ëŒ€ì‹œë³´ë“œ**: ì‹œê°í™”ë¥¼ ìœ„í•œ ì§‘ê³„ ë°ì´í„°

## ğŸ“Š ERD (Entity Relationship Diagram)

```mermaid
erDiagram
    crawling_metrics {
        bigint id PK
        bigint source_id FK
        bigint target_type_id FK
        varchar(50) metric_type "SUCCESS_RATE, SPEED, ERROR_RATE"
        date metric_date
        int hour "ì‹œê°„ (0-23)"
        bigint total_requests
        bigint successful_requests
        bigint failed_requests
        float avg_response_time_ms
        float p95_response_time_ms
        float p99_response_time_ms
        bigint total_bytes_downloaded
        bigint total_items_crawled
        timestamp created_at
    }
    
    error_logs {
        bigint id PK
        bigint job_id FK
        bigint task_id FK
        bigint source_id FK
        varchar(100) error_code
        varchar(200) error_type
        text error_message
        text stack_trace
        varchar(1000) request_url
        int status_code
        varchar(100) component "FETCHER, PARSER, STORAGE"
        enum severity "LOW, MEDIUM, HIGH, CRITICAL"
        boolean is_resolved
        timestamp occurred_at
        timestamp resolved_at
        timestamp created_at
    }
    
    system_health {
        bigint id PK
        varchar(100) component_name
        enum status "HEALTHY, DEGRADED, DOWN"
        float cpu_usage_percent
        float memory_usage_percent
        float disk_usage_percent
        bigint queue_depth
        int active_workers
        float avg_queue_wait_time_sec
        json health_checks
        timestamp checked_at
        timestamp created_at
    }
    
    data_quality_metrics {
        bigint id PK
        bigint source_id FK
        bigint target_type_id FK
        date check_date
        bigint total_records_checked
        bigint valid_records
        bigint invalid_records
        bigint missing_required_fields
        bigint duplicate_records
        float completeness_score "0-100"
        float accuracy_score "0-100"
        float consistency_score "0-100"
        json validation_errors
        timestamp checked_at
        timestamp created_at
    }
    
    alert_configurations {
        bigint id PK
        varchar(100) alert_name
        enum alert_type "THRESHOLD, ANOMALY, ERROR"
        varchar(100) metric_name
        enum condition_operator "GT, LT, EQ, GTE, LTE"
        float threshold_value
        int evaluation_period_minutes
        int consecutive_breaches
        enum severity "INFO, WARNING, ERROR, CRITICAL"
        varchar(100) notification_channel "EMAIL, SLACK, WEBHOOK"
        json notification_config
        boolean is_active
        timestamp created_at
        timestamp updated_at
    }
    
    alert_history {
        bigint id PK
        bigint alert_config_id FK
        enum status "TRIGGERED, RESOLVED, ACKNOWLEDGED"
        float metric_value
        float threshold_value
        text alert_message
        json alert_context
        timestamp triggered_at
        timestamp resolved_at
        timestamp acknowledged_at
        varchar(100) acknowledged_by
        timestamp created_at
    }
    
    performance_snapshots {
        bigint id PK
        timestamp snapshot_time
        int total_active_jobs
        int total_pending_jobs
        int total_failed_jobs
        float avg_job_duration_sec
        float system_cpu_percent
        float system_memory_percent
        bigint total_requests_per_min
        float avg_response_time_ms
        json detailed_metrics
        timestamp created_at
    }
    
    crawling_statistics {
        bigint id PK
        date stat_date
        varchar(50) stat_type "DAILY, WEEKLY, MONTHLY"
        bigint total_sources_active
        bigint total_targets_crawled
        bigint total_data_points_collected
        bigint total_bytes_processed
        float overall_success_rate
        float avg_crawl_duration_min
        json top_performing_sources
        json top_errors
        timestamp created_at
    }
    
    audit_logs {
        bigint id PK
        varchar(100) action_type "CONFIG_CHANGE, MANUAL_TRIGGER, SYSTEM_EVENT"
        varchar(100) entity_type "SOURCE, TARGET, WORKFLOW"
        bigint entity_id
        varchar(100) performed_by
        text action_description
        json old_value
        json new_value
        varchar(45) ip_address
        varchar(500) user_agent
        timestamp performed_at
        timestamp created_at
    }
    
    dashboard_widgets {
        bigint id PK
        varchar(100) widget_name
        varchar(50) widget_type "CHART, TABLE, METRIC, MAP"
        varchar(50) chart_type "LINE, BAR, PIE, GAUGE"
        text query_sql
        json display_config
        int refresh_interval_seconds
        int display_order
        boolean is_active
        timestamp created_at
        timestamp updated_at
    }
    
    alert_configurations ||--o{ alert_history : triggers
    crawling_sources ||--o{ crawling_metrics : measures
    crawling_sources ||--o{ error_logs : logs
    crawling_sources ||--o{ data_quality_metrics : validates
```

## ğŸ“ í…Œì´ë¸” ìƒì„¸ ì„¤ëª…

### 1. crawling_metrics (í¬ë¡¤ë§ ë©”íŠ¸ë¦­)

ì‹œê°„ë³„/ì¼ë³„ í¬ë¡¤ë§ ì„±ëŠ¥ ì§€í‘œ

#### ì£¼ìš” ë©”íŠ¸ë¦­
- **SUCCESS_RATE**: ì„±ê³µë¥ 
- **SPEED**: í¬ë¡¤ë§ ì†ë„ (items/min)
- **ERROR_RATE**: ì—ëŸ¬ìœ¨
- **RESPONSE_TIME**: ì‘ë‹µ ì‹œê°„ ë¶„í¬

#### ì§‘ê³„ ì˜ˆì‹œ
```sql
-- ì‹œê°„ë³„ ë©”íŠ¸ë¦­ ì§‘ê³„
INSERT INTO crawling_metrics (source_id, target_type_id, metric_date, hour, total_requests, successful_requests)
SELECT 
    source_id,
    t.type_id,
    DATE(requested_at) as metric_date,
    HOUR(requested_at) as hour,
    COUNT(*) as total_requests,
    SUM(CASE WHEN status_code BETWEEN 200 AND 299 THEN 1 ELSE 0 END) as successful_requests
FROM request_logs rl
JOIN crawling_targets t ON rl.target_id = t.id
WHERE requested_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY source_id, t.type_id, DATE(requested_at), HOUR(requested_at);
```

### 2. error_logs (ì—ëŸ¬ ë¡œê·¸)

ëª¨ë“  ì—ëŸ¬ ìƒì„¸ ê¸°ë¡

#### ì—ëŸ¬ ë¶„ë¥˜
- **Component**: FETCHER, PARSER, STORAGE
- **Severity**: LOW, MEDIUM, HIGH, CRITICAL

#### ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
```sql
-- ë¹ˆë°œ ì—ëŸ¬ íŒ¨í„´
SELECT 
    error_type,
    error_code,
    COUNT(*) as occurrence_count,
    AVG(CASE WHEN is_resolved THEN 1 ELSE 0 END) as resolution_rate
FROM error_logs
WHERE occurred_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
GROUP BY error_type, error_code
ORDER BY occurrence_count DESC;
```

### 3. system_health (ì‹œìŠ¤í…œ í—¬ìŠ¤)

ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ

#### í—¬ìŠ¤ ì²´í¬ í•­ëª©
```json
{
  "database": {
    "status": "healthy",
    "latency_ms": 5,
    "connection_pool_usage": 0.3
  },
  "redis": {
    "status": "healthy",
    "memory_usage_mb": 512,
    "hit_rate": 0.95
  },
  "s3": {
    "status": "healthy",
    "latency_ms": 50
  },
  "queue": {
    "status": "degraded",
    "depth": 5000,
    "processing_rate": 100
  }
}
```

### 4. data_quality_metrics (ë°ì´í„° í’ˆì§ˆ)

ìˆ˜ì§‘ ë°ì´í„°ì˜ í’ˆì§ˆ í‰ê°€

#### í’ˆì§ˆ ì ìˆ˜
- **Completeness**: í•„ìˆ˜ í•„ë“œ ì™„ì „ì„± (0-100)
- **Accuracy**: ë°ì´í„° ì •í™•ì„± (0-100)
- **Consistency**: ë°ì´í„° ì¼ê´€ì„± (0-100)

#### ê²€ì¦ ê·œì¹™
```sql
-- ë°ì´í„° í’ˆì§ˆ ì²´í¬
CREATE PROCEDURE check_data_quality(IN p_source_id BIGINT, IN p_date DATE)
BEGIN
    DECLARE v_total_records BIGINT;
    DECLARE v_missing_required BIGINT;
    DECLARE v_duplicates BIGINT;
    
    -- í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ì²´í¬
    SELECT COUNT(*), 
           SUM(CASE WHEN value_text IS NULL AND ta.is_required THEN 1 ELSE 0 END)
    INTO v_total_records, v_missing_required
    FROM crawling_data_values cdv
    JOIN target_attributes ta ON cdv.attribute_id = ta.id
    WHERE DATE(cdv.created_at) = p_date;
    
    -- ì¤‘ë³µ ì²´í¬
    SELECT COUNT(*) INTO v_duplicates
    FROM (
        SELECT target_id, version_hash, COUNT(*) as cnt
        FROM crawling_data
        WHERE DATE(crawled_at) = p_date
        GROUP BY target_id, version_hash
        HAVING cnt > 1
    ) dups;
    
    -- ë©”íŠ¸ë¦­ ì €ì¥
    INSERT INTO data_quality_metrics (
        source_id, check_date, total_records_checked,
        missing_required_fields, duplicate_records,
        completeness_score
    ) VALUES (
        p_source_id, p_date, v_total_records,
        v_missing_required, v_duplicates,
        (1 - v_missing_required / v_total_records) * 100
    );
END;
```

### 5. alert_configurations (ì•Œë¦¼ ì„¤ì •)

ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ ê·œì¹™

#### ì•Œë¦¼ íƒ€ì…
- **THRESHOLD**: ì„ê³„ê°’ ì´ˆê³¼/ë¯¸ë‹¬
- **ANOMALY**: ì´ìƒ íŒ¨í„´ ê°ì§€
- **ERROR**: ì—ëŸ¬ ë°œìƒ

#### ì•Œë¦¼ ì„¤ì • ì˜ˆì‹œ
```sql
-- ì„±ê³µë¥  70% ë¯¸ë§Œ ì•Œë¦¼
INSERT INTO alert_configurations (
    alert_name, alert_type, metric_name, 
    condition_operator, threshold_value,
    evaluation_period_minutes, severity
) VALUES (
    'ë‚®ì€ ì„±ê³µë¥ ', 'THRESHOLD', 'success_rate',
    'LT', 70.0,
    5, 'WARNING'
);

-- ì—ëŸ¬ìœ¨ 10% ì´ˆê³¼ ì•Œë¦¼
INSERT INTO alert_configurations (
    alert_name, alert_type, metric_name,
    condition_operator, threshold_value,
    evaluation_period_minutes, severity
) VALUES (
    'ë†’ì€ ì—ëŸ¬ìœ¨', 'THRESHOLD', 'error_rate',
    'GT', 10.0,
    5, 'ERROR'
);
```

### 6. alert_history (ì•Œë¦¼ ì´ë ¥)

ë°œìƒí•œ ì•Œë¦¼ì˜ ì´ë ¥ ê´€ë¦¬

#### ì•Œë¦¼ ìƒíƒœ íë¦„
```
TRIGGERED â†’ ACKNOWLEDGED â†’ RESOLVED
```

### 7. performance_snapshots (ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ·)

ì‹œìŠ¤í…œ ì „ì²´ ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· (1ë¶„ ê°„ê²©)

#### ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘
```sql
-- 1ë¶„ë§ˆë‹¤ ìŠ¤ëƒ…ìƒ· ìƒì„±
CREATE EVENT collect_performance_snapshot
ON SCHEDULE EVERY 1 MINUTE
DO
    INSERT INTO performance_snapshots (
        snapshot_time,
        total_active_jobs,
        total_pending_jobs,
        avg_job_duration_sec,
        total_requests_per_min
    )
    SELECT 
        NOW(),
        SUM(CASE WHEN status = 'RUNNING' THEN 1 ELSE 0 END),
        SUM(CASE WHEN status = 'PENDING' THEN 1 ELSE 0 END),
        AVG(TIMESTAMPDIFF(SECOND, started_at, completed_at)),
        (SELECT COUNT(*) FROM request_logs WHERE requested_at >= DATE_SUB(NOW(), INTERVAL 1 MINUTE))
    FROM crawling_jobs
    WHERE created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR);
```

### 8. crawling_statistics (í¬ë¡¤ë§ í†µê³„)

ì¼/ì£¼/ì›” ë‹¨ìœ„ ì¢…í•© í†µê³„

#### í†µê³„ ì§‘ê³„
```sql
-- ì¼ë³„ í†µê³„ ì§‘ê³„
CREATE PROCEDURE generate_daily_statistics(IN p_date DATE)
BEGIN
    INSERT INTO crawling_statistics (
        stat_date, stat_type,
        total_sources_active,
        total_targets_crawled,
        total_data_points_collected,
        overall_success_rate
    )
    SELECT 
        p_date, 'DAILY',
        COUNT(DISTINCT source_id),
        COUNT(DISTINCT target_id),
        SUM(processed_items),
        AVG(success_count / NULLIF(processed_items, 0))
    FROM crawling_jobs
    WHERE DATE(created_at) = p_date;
END;
```

### 9. audit_logs (ê°ì‚¬ ë¡œê·¸)

ì‹œìŠ¤í…œ ë³€ê²½ ì‚¬í•­ ì¶”ì 

#### ê°ì‚¬ ëŒ€ìƒ
- ì„¤ì • ë³€ê²½
- ìˆ˜ë™ ì‘ì—… ì‹¤í–‰
- ê¶Œí•œ ë³€ê²½
- ì¤‘ìš” ì‹œìŠ¤í…œ ì´ë²¤íŠ¸

### 10. dashboard_widgets (ëŒ€ì‹œë³´ë“œ ìœ„ì ¯)

ëŒ€ì‹œë³´ë“œ êµ¬ì„±ì„ ìœ„í•œ ìœ„ì ¯ ì •ì˜

#### ìœ„ì ¯ íƒ€ì…
- **CHART**: ì°¨íŠ¸ (ë¼ì¸, ë°”, íŒŒì´)
- **TABLE**: í…Œì´ë¸”
- **METRIC**: ë‹¨ì¼ ì§€í‘œ
- **MAP**: ì§€ë„/íˆíŠ¸ë§µ

#### ìœ„ì ¯ ì„¤ì • ì˜ˆì‹œ
```json
{
  "title": "í¬ë¡¤ë§ ì„±ê³µë¥  ì¶”ì´",
  "type": "LINE",
  "x_axis": "time",
  "y_axis": "success_rate",
  "period": "24h",
  "refresh": 60,
  "colors": ["#28a745", "#dc3545"]
}
```

## ğŸ“ˆ ì£¼ìš” ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

### 1. ì‹¤ì‹œê°„ í˜„í™©
```sql
-- ì‹¤ì‹œê°„ í¬ë¡¤ë§ í˜„í™©
CREATE VIEW v_realtime_status AS
SELECT 
    (SELECT COUNT(*) FROM crawling_jobs WHERE status = 'RUNNING') as active_jobs,
    (SELECT COUNT(*) FROM crawling_jobs WHERE status = 'PENDING') as pending_jobs,
    (SELECT AVG(success_rate) FROM crawling_metrics WHERE metric_date = CURDATE()) as today_success_rate,
    (SELECT COUNT(*) FROM error_logs WHERE occurred_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR) AND severity IN ('HIGH', 'CRITICAL')) as recent_critical_errors,
    (SELECT COUNT(*) FROM alert_history WHERE triggered_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR) AND status = 'TRIGGERED') as active_alerts;
```

### 2. ì†ŒìŠ¤ë³„ ì„±ëŠ¥
```sql
-- ì†ŒìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ
CREATE VIEW v_source_performance AS
SELECT 
    s.name as source_name,
    cm.metric_date,
    cm.successful_requests / NULLIF(cm.total_requests, 0) * 100 as success_rate,
    cm.avg_response_time_ms,
    cm.total_items_crawled,
    dq.completeness_score,
    dq.accuracy_score
FROM crawling_sources s
LEFT JOIN crawling_metrics cm ON s.id = cm.source_id
LEFT JOIN data_quality_metrics dq ON s.id = dq.source_id AND cm.metric_date = dq.check_date
WHERE cm.metric_date >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
ORDER BY cm.metric_date DESC, success_rate DESC;
```

### 3. ì—ëŸ¬ ë¶„ì„
```sql
-- Top 10 ì—ëŸ¬ ìœ í˜•
CREATE VIEW v_top_errors AS
SELECT 
    error_type,
    error_code,
    component,
    COUNT(*) as error_count,
    MAX(occurred_at) as last_occurred,
    AVG(CASE WHEN is_resolved THEN 
        TIMESTAMPDIFF(MINUTE, occurred_at, resolved_at) 
    END) as avg_resolution_time_min
FROM error_logs
WHERE occurred_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
GROUP BY error_type, error_code, component
ORDER BY error_count DESC
LIMIT 10;
```

## ğŸš¨ ì•Œë¦¼ ê·œì¹™

### 1. í¬ë¦¬í‹°ì»¬ ì•Œë¦¼
```sql
-- ì‹œìŠ¤í…œ ë‹¤ìš´
INSERT INTO alert_configurations VALUES 
(NULL, 'ì‹œìŠ¤í…œ ë‹¤ìš´', 'THRESHOLD', 'system_health_status', 'EQ', 0, 1, 3, 'CRITICAL', 'SLACK', '{"channel": "#alerts"}', TRUE, NOW(), NOW());

-- í ê³¼ë¶€í•˜
INSERT INTO alert_configurations VALUES
(NULL, 'í ê³¼ë¶€í•˜', 'THRESHOLD', 'queue_depth', 'GT', 10000, 5, 2, 'ERROR', 'EMAIL', '{"to": "ops@example.com"}', TRUE, NOW(), NOW());
```

### 2. ì„±ëŠ¥ ì•Œë¦¼
```sql
-- ëŠë¦° ì‘ë‹µ ì‹œê°„
INSERT INTO alert_configurations VALUES
(NULL, 'ëŠë¦° ì‘ë‹µ', 'THRESHOLD', 'p95_response_time', 'GT', 5000, 10, 3, 'WARNING', 'SLACK', '{"channel": "#performance"}', TRUE, NOW(), NOW());
```

## ğŸ“Š ë¦¬í¬íŒ…

### ì¼ì¼ ë¦¬í¬íŠ¸
```sql
-- ì¼ì¼ í¬ë¡¤ë§ ìš”ì•½ ë¦¬í¬íŠ¸
CREATE PROCEDURE generate_daily_report(IN p_date DATE)
BEGIN
    SELECT 
        'Daily Crawling Report' as report_title,
        p_date as report_date,
        (SELECT COUNT(DISTINCT source_id) FROM crawling_jobs WHERE DATE(created_at) = p_date) as active_sources,
        (SELECT COUNT(*) FROM crawling_jobs WHERE DATE(created_at) = p_date) as total_jobs,
        (SELECT SUM(processed_items) FROM crawling_jobs WHERE DATE(created_at) = p_date) as total_items,
        (SELECT AVG(success_count/NULLIF(processed_items,0))*100 FROM crawling_jobs WHERE DATE(created_at) = p_date) as avg_success_rate,
        (SELECT COUNT(*) FROM error_logs WHERE DATE(occurred_at) = p_date AND severity IN ('HIGH', 'CRITICAL')) as critical_errors,
        (SELECT AVG(completeness_score) FROM data_quality_metrics WHERE check_date = p_date) as avg_data_quality;
END;
```

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ
- [00-overview.md](00-overview.md) - ì „ì²´ ê°œìš”
- [02-workflow-domain.md](02-workflow-domain.md) - ì›Œí¬í”Œë¡œìš° ë„ë©”ì¸
- [03-security-domain.md](03-security-domain.md) - ë³´ì•ˆ ë„ë©”ì¸
