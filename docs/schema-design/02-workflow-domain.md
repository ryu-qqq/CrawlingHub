# ğŸ”„ ì›Œí¬í”Œë¡œìš° ë„ë©”ì¸ (Workflow Domain) ìŠ¤í‚¤ë§ˆ ì„¤ê³„

## ğŸ“Œ ê°œìš”

ì›Œí¬í”Œë¡œìš° ë„ë©”ì¸ì€ í¬ë¡¤ë§ ì‘ì—…ì˜ ì‹¤í–‰ê³¼ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
ë²”ìš© íƒ€ê²Ÿ ì‹œìŠ¤í…œê³¼ ì—°ë™í•˜ì—¬ ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

### í•µì‹¬ ê°œë…
- **Workflow**: í¬ë¡¤ë§ ì‘ì—… íë¦„ ì •ì˜
- **Job**: ì‹¤í–‰ ë‹¨ìœ„ ì‘ì—…
- **Task**: ì„¸ë¶€ ì‹¤í–‰ íƒœìŠ¤í¬
- **Schedule**: ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- **Queue**: ì‘ì—… í ê´€ë¦¬

## ğŸ“Š ERD (Entity Relationship Diagram)

```mermaid
erDiagram
    crawling_workflows {
        bigint id PK
        bigint source_id FK
        varchar(100) workflow_name "ì›Œí¬í”Œë¡œìš° ì´ë¦„"
        enum workflow_type "FULL_SCAN, INCREMENTAL, TARGETED"
        text workflow_definition "ì›Œí¬í”Œë¡œìš° ì •ì˜"
        boolean is_active
        int version "ë²„ì „"
        timestamp created_at
        timestamp updated_at
    }
    
    workflow_steps {
        bigint id PK
        bigint workflow_id FK
        int step_order "ì‹¤í–‰ ìˆœì„œ"
        varchar(100) step_name "ë‹¨ê³„ ì´ë¦„"
        enum step_type "FETCH, PARSE, TRANSFORM, VALIDATE, STORE"
        bigint target_type_id FK "ëŒ€ìƒ íƒ€ì…"
        text input_mapping "ì…ë ¥ ë§¤í•‘"
        text output_mapping "ì¶œë ¥ ë§¤í•‘"
        enum error_handling "RETRY, SKIP, FAIL"
        int max_retries "ìµœëŒ€ ì¬ì‹œë„"
        timestamp created_at
    }
    
    crawling_schedules {
        bigint id PK
        bigint workflow_id FK
        bigint target_id FK "í¬ë¡¤ë§ ëŒ€ìƒ"
        enum schedule_type "FIXED, DYNAMIC, MANUAL"
        varchar(100) cron_expression "í¬ë¡  í‘œí˜„ì‹"
        boolean is_active
        int priority "ìš°ì„ ìˆœìœ„ 1-10"
        timestamp next_run_at "ë‹¤ìŒ ì‹¤í–‰"
        timestamp last_run_at "ë§ˆì§€ë§‰ ì‹¤í–‰"
        timestamp created_at
        timestamp updated_at
        timestamp deleted_at
    }
    
    crawling_jobs {
        bigint id PK
        bigint schedule_id FK
        bigint workflow_id FK
        bigint target_id FK
        enum job_type "FULL, INCREMENTAL, RETRY"
        enum status "PENDING, RUNNING, SUCCESS, FAILED, CANCELLED"
        timestamp scheduled_at
        timestamp started_at
        timestamp completed_at
        int total_items "ì´ í•­ëª© ìˆ˜"
        int processed_items "ì²˜ë¦¬ëœ í•­ëª©"
        int success_count
        int failed_count
        text error_summary
        timestamp created_at
        timestamp updated_at
    }
    
    crawling_tasks {
        bigint id PK
        bigint job_id FK
        bigint step_id FK "ì›Œí¬í”Œë¡œìš° ë‹¨ê³„"
        varchar(1000) target_url
        enum task_type "FETCH, PARSE, STORE"
        enum status "PENDING, RUNNING, SUCCESS, FAILED, RETRYING"
        int retry_count
        int sequence_number "ìˆœì„œ ë²ˆí˜¸"
        text input_data "ì…ë ¥ ë°ì´í„°"
        text output_data "ì¶œë ¥ ë°ì´í„°"
        timestamp started_at
        timestamp completed_at
        int response_time_ms
        text error_message
        timestamp created_at
        timestamp updated_at
    }
    
    crawling_queues {
        bigint id PK
        enum queue_type "HIGH_PRIORITY, NORMAL, LOW_PRIORITY"
        bigint job_id FK
        bigint task_id FK
        int priority "1-10"
        enum status "WAITING, PROCESSING, COMPLETED, FAILED"
        timestamp scheduled_for
        int attempt_count
        text payload "ì‘ì—… ë°ì´í„°"
        timestamp created_at
        timestamp updated_at
    }
    
    workflow_executions {
        bigint id PK
        bigint workflow_id FK
        bigint job_id FK
        varchar(50) current_step
        enum status "RUNNING, COMPLETED, FAILED, PAUSED"
        text context "ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"
        text step_results "ë‹¨ê³„ë³„ ê²°ê³¼"
        timestamp started_at
        timestamp completed_at
        timestamp created_at
    }
    
    crawling_workflows ||--o{ workflow_steps : contains
    crawling_workflows ||--o{ crawling_schedules : scheduled_by
    crawling_workflows ||--o{ crawling_jobs : executes
    crawling_schedules ||--o{ crawling_jobs : triggers
    crawling_jobs ||--o{ crawling_tasks : contains
    crawling_jobs ||--o{ workflow_executions : tracked_by
    workflow_steps ||--o{ crawling_tasks : defines
    crawling_tasks }o--o{ crawling_queues : queued_in
```

## ğŸ“ í…Œì´ë¸” ìƒì„¸ ì„¤ëª…

### 1. crawling_workflows (ì›Œí¬í”Œë¡œìš° ì •ì˜)

í¬ë¡¤ë§ ì‘ì—… íë¦„ì„ ì •ì˜í•˜ëŠ” ë§ˆìŠ¤í„° í…Œì´ë¸”

#### ì£¼ìš” ì»¬ëŸ¼
- **workflow_type**: ì›Œí¬í”Œë¡œìš° ìœ í˜•
  - `FULL_SCAN`: ì „ì²´ ìŠ¤ìº”
  - `INCREMENTAL`: ì¦ë¶„ ì—…ë°ì´íŠ¸
  - `TARGETED`: íŠ¹ì • ëŒ€ìƒë§Œ
- **workflow_definition**: JSON í˜•íƒœì˜ ì›Œí¬í”Œë¡œìš° ì •ì˜
  ```json
  {
    "steps": [
      {
        "name": "fetch_list",
        "type": "FETCH",
        "config": {
          "url_pattern": "{base_url}/list?page={page}",
          "pagination": true,
          "max_pages": 100
        }
      },
      {
        "name": "parse_items",
        "type": "PARSE",
        "config": {
          "parser": "html",
          "selectors": {
            "items": ".product-item",
            "name": ".title",
            "price": ".price"
          }
        }
      }
    ]
  }
  ```

#### ì˜ˆì‹œ ë°ì´í„°
```sql
INSERT INTO crawling_workflows (source_id, workflow_name, workflow_type) VALUES
(@mustit_id, 'MUSTIT ìƒí’ˆ ìˆ˜ì§‘', 'INCREMENTAL'),
(@naver_id, 'ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘', 'FULL_SCAN'),
(@instagram_id, 'ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘', 'TARGETED');
```

### 2. workflow_steps (ì›Œí¬í”Œë¡œìš° ë‹¨ê³„)

ì›Œí¬í”Œë¡œìš°ì˜ ê° ì‹¤í–‰ ë‹¨ê³„ë¥¼ ì •ì˜

#### ë‹¨ê³„ íƒ€ì…
- **FETCH**: ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
- **PARSE**: íŒŒì‹±/ì¶”ì¶œ
- **TRANSFORM**: ë°ì´í„° ë³€í™˜
- **VALIDATE**: ê²€ì¦
- **STORE**: ì €ì¥

#### ì—ëŸ¬ ì²˜ë¦¬ ì „ëµ
- **RETRY**: ì¬ì‹œë„
- **SKIP**: ê±´ë„ˆë›°ê¸°
- **FAIL**: ì‹¤íŒ¨ ì²˜ë¦¬

### 3. crawling_schedules (ìŠ¤ì¼€ì¤„ë§)

í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ ê´€ë¦¬

#### ìŠ¤ì¼€ì¤„ íƒ€ì…
- **FIXED**: ê³ ì • ì£¼ê¸° (í¬ë¡ )
- **DYNAMIC**: ë™ì  ì¡°ì •
- **MANUAL**: ìˆ˜ë™ ì‹¤í–‰

#### í¬ë¡  í‘œí˜„ì‹ ì˜ˆì‹œ
```sql
-- ë§¤ì¼ ìƒˆë²½ 2ì‹œ
'0 2 * * *'

-- 30ë¶„ë§ˆë‹¤
'*/30 * * * *'

-- í‰ì¼ ì˜¤ì „ 9ì‹œ
'0 9 * * 1-5'
```

### 4. crawling_jobs (í¬ë¡¤ë§ ì‘ì—…)

ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ ì™„ë£Œëœ í¬ë¡¤ë§ ì‘ì—…

#### ì‘ì—… ìƒíƒœ íë¦„
```
PENDING â†’ RUNNING â†’ SUCCESS
                  â†˜ FAILED
                  â†˜ CANCELLED
```

#### í†µê³„ ì •ë³´
- **total_items**: ì²˜ë¦¬í•  ì´ í•­ëª© ìˆ˜
- **processed_items**: ì²˜ë¦¬ ì™„ë£Œ í•­ëª©
- **success_count**: ì„±ê³µ ê°œìˆ˜
- **failed_count**: ì‹¤íŒ¨ ê°œìˆ˜

### 5. crawling_tasks (ì„¸ë¶€ íƒœìŠ¤í¬)

ì‘ì—…ì˜ ì„¸ë¶€ ì‹¤í–‰ ë‹¨ìœ„

#### ì£¼ìš” ê¸°ëŠ¥
- **ì¬ì‹œë„ ê´€ë¦¬**: retry_countë¡œ ì¬ì‹œë„ ì¶”ì 
- **ìˆœì„œ ë³´ì¥**: sequence_numberë¡œ ì‹¤í–‰ ìˆœì„œ ê´€ë¦¬
- **ì„±ëŠ¥ ì¶”ì **: response_time_msë¡œ ì‘ë‹µ ì‹œê°„ ì¸¡ì •

#### ì¸ë±ìŠ¤
```sql
CREATE INDEX idx_tasks_job_status ON crawling_tasks(job_id, status);
CREATE INDEX idx_tasks_retry ON crawling_tasks(status, retry_count) 
  WHERE status = 'FAILED' AND retry_count < 3;
```

### 6. crawling_queues (ì‘ì—… í)

ë¹„ë™ê¸° ì‘ì—… í ê´€ë¦¬

#### í ìš°ì„ ìˆœìœ„
- **HIGH_PRIORITY**: ì¦‰ì‹œ ì‹¤í–‰
- **NORMAL**: ì¼ë°˜ ìš°ì„ ìˆœìœ„
- **LOW_PRIORITY**: ìœ íœ´ ì‹œê°„ ì‹¤í–‰

#### í ì²˜ë¦¬ ë¡œì§
```sql
-- ë‹¤ìŒ ì‹¤í–‰í•  ì‘ì—… ì„ íƒ
SELECT * FROM crawling_queues
WHERE status = 'WAITING'
  AND scheduled_for <= NOW()
ORDER BY queue_type, priority DESC, created_at
LIMIT 10
FOR UPDATE SKIP LOCKED;
```

### 7. workflow_executions (ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¶”ì )

ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ìƒíƒœ ì¶”ì 

#### ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì˜ˆì‹œ
```json
{
  "current_page": 5,
  "total_pages": 100,
  "items_processed": 300,
  "last_item_id": "PROD_12345",
  "checkpoint": {
    "step": "parse_items",
    "progress": 0.05
  }
}
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### 1. ë™ì  ì›Œí¬í”Œë¡œìš°

```sql
-- íƒ€ì…ë³„ ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš° ì ìš©
CREATE TABLE target_workflow_mappings (
    target_type_id BIGINT,
    workflow_id BIGINT,
    condition_expression TEXT, -- ì¡°ê±´ì‹
    priority INT,
    PRIMARY KEY (target_type_id, workflow_id)
);
```

### 2. ì˜ì¡´ì„± ê´€ë¦¬

```sql
-- ì‘ì—… ê°„ ì˜ì¡´ì„±
CREATE TABLE job_dependencies (
    job_id BIGINT,
    depends_on_job_id BIGINT,
    dependency_type ENUM('FINISH', 'SUCCESS'),
    PRIMARY KEY (job_id, depends_on_job_id)
);
```

### 3. ë³‘ë ¬ ì‹¤í–‰

```sql
-- ë³‘ë ¬ ì‹¤í–‰ ì„¤ì •
CREATE TABLE parallel_execution_config (
    workflow_id BIGINT PRIMARY KEY,
    max_parallel_jobs INT DEFAULT 5,
    max_parallel_tasks INT DEFAULT 10,
    resource_pool VARCHAR(50)
);
```

## ğŸ”„ ì‹¤í–‰ íë¦„

### 1. ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ì‹¤í–‰
```sql
-- 1. ì‹¤í–‰ ëŒ€ìƒ ìŠ¤ì¼€ì¤„ í™•ì¸
SELECT * FROM crawling_schedules
WHERE is_active = TRUE
  AND next_run_at <= NOW()
ORDER BY priority DESC;

-- 2. Job ìƒì„±
INSERT INTO crawling_jobs (schedule_id, workflow_id, target_id, status)
VALUES (@schedule_id, @workflow_id, @target_id, 'PENDING');

-- 3. Task ìƒì„±
INSERT INTO crawling_tasks (job_id, step_id, target_url, status)
SELECT @job_id, ws.id, CONCAT(@base_url, ws.url_pattern), 'PENDING'
FROM workflow_steps ws
WHERE workflow_id = @workflow_id
ORDER BY step_order;

-- 4. í ë“±ë¡
INSERT INTO crawling_queues (task_id, queue_type, priority, status)
SELECT id, 'NORMAL', 5, 'WAITING'
FROM crawling_tasks
WHERE job_id = @job_id;
```

### 2. ì‘ì—… ì‹¤í–‰ ë° ì™„ë£Œ
```sql
-- ì‘ì—… ì‹œì‘
UPDATE crawling_jobs 
SET status = 'RUNNING', started_at = NOW()
WHERE id = @job_id;

-- íƒœìŠ¤í¬ ì²˜ë¦¬
UPDATE crawling_tasks
SET status = 'SUCCESS', 
    completed_at = NOW(),
    response_time_ms = TIMESTAMPDIFF(MICROSECOND, started_at, NOW()) / 1000
WHERE id = @task_id;

-- ì‘ì—… ì™„ë£Œ
UPDATE crawling_jobs
SET status = 'SUCCESS',
    completed_at = NOW(),
    processed_items = (SELECT COUNT(*) FROM crawling_tasks WHERE job_id = @job_id),
    success_count = (SELECT COUNT(*) FROM crawling_tasks WHERE job_id = @job_id AND status = 'SUCCESS')
WHERE id = @job_id;
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. í íŒŒí‹°ì…”ë‹
```sql
-- í í…Œì´ë¸” íŒŒí‹°ì…”ë‹ (ì¼ë³„)
ALTER TABLE crawling_queues
PARTITION BY RANGE (TO_DAYS(created_at)) (
    PARTITION p_today VALUES LESS THAN (TO_DAYS(CURDATE() + INTERVAL 1 DAY)),
    PARTITION p_tomorrow VALUES LESS THAN (TO_DAYS(CURDATE() + INTERVAL 2 DAY)),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

### 2. ì‘ì—… ì•„ì¹´ì´ë¹™
```sql
-- ì™„ë£Œëœ ì‘ì—… ì•„ì¹´ì´ë¹™ (30ì¼ ì´ìƒ)
INSERT INTO crawling_jobs_archive
SELECT * FROM crawling_jobs
WHERE completed_at < DATE_SUB(NOW(), INTERVAL 30 DAY);

DELETE FROM crawling_jobs
WHERE completed_at < DATE_SUB(NOW(), INTERVAL 30 DAY);
```

### 3. í†µê³„ ì§‘ê³„
```sql
-- ì‹¤ì‹œê°„ í†µê³„ View
CREATE VIEW v_job_statistics AS
SELECT 
    DATE(created_at) as job_date,
    workflow_id,
    COUNT(*) as total_jobs,
    SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as success_jobs,
    AVG(TIMESTAMPDIFF(SECOND, started_at, completed_at)) as avg_duration_sec,
    SUM(processed_items) as total_items_processed
FROM crawling_jobs
WHERE created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)
GROUP BY DATE(created_at), workflow_id;
```

## ğŸ¯ ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸

### ì£¼ìš” ì§€í‘œ
1. **Job Success Rate**: ì‘ì—… ì„±ê³µë¥ 
2. **Average Execution Time**: í‰ê·  ì‹¤í–‰ ì‹œê°„
3. **Queue Depth**: í ëŒ€ê¸° ê¹Šì´
4. **Retry Rate**: ì¬ì‹œë„ ë¹„ìœ¨

### ì•Œë¦¼ ì¡°ê±´
```sql
-- ì‹¤íŒ¨ìœ¨ì´ ë†’ì€ ì›Œí¬í”Œë¡œìš°
SELECT workflow_id, 
       COUNT(*) as total,
       SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed,
       (SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) / COUNT(*)) * 100 as failure_rate
FROM crawling_jobs
WHERE created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY workflow_id
HAVING failure_rate > 20;
```

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ
- [00-overview.md](00-overview.md) - ì „ì²´ ê°œìš”
- [01-target-domain.md](01-target-domain.md) - íƒ€ê²Ÿ ë„ë©”ì¸
- [03-security-domain.md](03-security-domain.md) - ë³´ì•ˆ ë„ë©”ì¸
